// KQL Sample code - Step 5 Time series reporting
 
// This Sample Code is provided for the purpose of illustration only and is not intended to be used in a production environment.  
// THIS SAMPLE CODE AND ANY RELATED INFORMATION ARE PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY AND/OR FITNESS FOR A PARTICULAR PURPOSE.  
// We grant You a nonexclusive, royalty-free right to use and modify the 
// Sample Code and to reproduce and distribute the object code form of the Sample Code, provided that You agree: (i) to not use Our name, logo, or trademarks to market Your software product in which the Sample Code is embedded; 
// (ii) to include a valid copyright notice on Your software product in which the Sample Code is 
// embedded; and 
// (iii) to indemnify, hold harmless, and defend Us and Our suppliers from and against any claims or lawsuits, including attorneysâ€™ fees, that arise or result from the use or distribution of the Sample Code.
// Please note: None of the conditions outlined in the disclaimer above will supercede the terms and conditions contained within the Premier Customer Services Description.
// 


// range
// Produces a table in steps using the boundaries indicated, incrementing by 
// the value in the step parameter
range integers from 1 to 8 step 1;

range integers from 1 to 8 step 2;

// Works great with dates
range LastWeek from ago(7d) to now() step 1d;

range LastHours from ago(24h) to now() step 1h;


// Here, we'll create a table from a range, one for each day, for the
// start of the day. Then we'll join that to a query getting the 
// number of events bucketed by date
// We'll then add a column that nicely formats the date 
// It then sorts the results and displays them

// I will demo this by using the LogAnalytics demo site
//https://portal.loganalytics.io/demo
range LastWeek from ago(7d) to now() step 1d
| project TimeGenerated = startofday(LastWeek)
| join kind=fullouter ( Perf 
                      | where TimeGenerated > ago(7d)
                      | summarize Count = count() 
                               by bin(TimeGenerated, 1d)
) on TimeGenerated
| extend TimeDisplay = format_datetime(TimeGenerated, "yyyy-MM-dd") 
| sort by TimeGenerated desc
| project TimeDisplay 
        , Count 
| render timechart;


// make-series
// Takes a series of values and converts them to an array of values within a
// column
Perf
| where TimeGenerated > ago(3d)
| where CounterName  == "Available MBytes"
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(ago(3d), now(), 1h) by Computer;


// We can use make series to generate an array of averages and times, then
// use mvexpand to pivot them back to rows
let startTime=ago(2hour);
let endTime=now();
Perf 
| where TimeGenerated between (startTime .. endTime)
| where CounterName == "% Processor Time" 
    and Computer == "ContosoWeb1.ContosoRetail.com" 
    and ObjectName == "Process" 
    and InstanceName !in ("_Total", "Idle") 
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(startTime, endTime, 10m) 
           by InstanceName
| mvexpand TimeGenerated to typeof(datetime)
         , avg_CounterValue to typeof(double) 
           limit 100000;


// This query can then be used to render a chart over time
let startTime=ago(2hour);
let endTime=now();
Perf 
| where TimeGenerated between (startTime .. endTime)
| where CounterName == "% Processor Time" 
    and Computer == "ContosoWeb1.ContosoRetail.com" 
    and ObjectName == "Process" 
    and InstanceName !in ("_Total", "Idle") 
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(startTime, endTime, 10m) 
           by InstanceName
| mvexpand TimeGenerated to typeof(datetime)
         , avg_CounterValue to typeof(double) 
           limit 100000
| render timechart;

// Even better, render has the ability to handle expanding on it's own,
// so the mvexpand step can be eliminated
let startTime=ago(2hour);
let endTime=now();
Perf 
| where TimeGenerated between (startTime .. endTime)
| where CounterName == "% Processor Time" 
    and Computer == "ContosoWeb1.ContosoRetail.com" 
    and ObjectName == "Process" 
    and InstanceName !in ("_Total", "Idle") 
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(startTime, endTime, 10m) 
           by InstanceName
| render timechart;


// series_stats
// Takes a dynamic series of values and produces a list of all the statistical
// functions for them in one output
let x=dynamic([23,46,23,87,4,8,3,75,2,56,13,75,32,16,29]);
print series_stats(x);

// A variant is series_stats_dynamic, which returns a dynamic column with the
// data in json format
let x=dynamic([23,46,23,87,4,8,3,75,2,56,13,75,32,16,29]);
print series_stats_dynamic(x);


// series_stats_dynamic is used in conjuction with make-series to
// return statistics for a value
Perf
| where TimeGenerated > ago(1d)
| where CounterName == "Available MBytes"
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(ago(1d), now(), 1h) 
           by Computer
| extend mySeriesStats = series_stats_dynamic(avg_CounterValue); 

// Make it easier to read by projecting the individual values from the
// dyamic value
Perf
| where TimeGenerated > ago(1d)
| where CounterName == "Available MBytes"
| make-series avg(CounterValue) default=0 
           on TimeGenerated in range(ago(1d), now(), 1h) 
           by Computer
| extend mySeriesStats = series_stats_dynamic(avg_CounterValue) 
| project Computer
        , mySeriesStats.min
        , mySeriesStats.min_idx
        , mySeriesStats.max
        , mySeriesStats.max_idx
        , mySeriesStats.avg
        , mySeriesStats.stdev
        , mySeriesStats.variance;

// series_outliers
// For each element in the array that is passed in, series outliers generates
// a corresponding value that indicates the possiblity of an anomoly.
// A value greater than 1.5, or less than -1.5, indicates the rise or 
// decline of an anomoly. 


// Here, we'll use series-outliers to get accounts that suddenly log on 
// to more distinct machines than usual and perform admin detection

// The ultimate query has a lot of parts, so lets break it into small
// parts first. This is a common development scenerio, create the parts
// of the query first so you can test those components then combine
// them together

// Start by getting base data. For user login events, get the number
// of users that logged in (dcount will distinctly count the users)
let fromDate = ago(5d);
let thruDate = now();
let baseData = materialize(
    SecurityEvent
      // 4624 - An account was successfully logged on.
    | where EventID == 4624  
      // Only for user logins
    | where AccountType == "User"
      // Get a list of distinct logins by account and put them
      // in an array variable
    | make-series dcount(Computer) default=0 
               on TimeGenerated in range(fromDate, thruDate, 1d) 
               by Account 
);
baseData;

// Now we'll get a list of outliers - accounts with logins that exceeded
// the normal login rates for that account
let fromDate = ago(5d);
let thruDate = now();
let baseData = materialize(
    SecurityEvent
    | where EventID == 4624  
    | where AccountType == "User"
    | make-series dcount(Computer) default=0 
               on TimeGenerated in range(fromDate, thruDate, 1d) 
               by Account 
);
// Get the top 25 ranked by outlier
let AnomAccounts = (
    baseData
      // Calculate the outliers into an array
    | extend outliers = series_outliers(dcount_Computer) 
      // Now convert the array into rows
    | mvexpand dcount_Computer, TimeGenerated, outliers to typeof(double)
      // Get just the top 25 ranked by outlier
    | top 25 by outliers desc
    // We will add this later to just get the list of accounts, for now
    // we comment it out so we can verify the full results 
//    | project Account  
);
AnomAccounts;


// Next get a list of the average number of logins for the user account
let fromDate = ago(5d);
let thruDate = now();
let baseData = materialize(
    SecurityEvent
    | where EventID == 4624  
    | where AccountType == "User"
    | make-series dcount(Computer) default=0 
               on TimeGenerated in range(fromDate, thruDate, 1d) 
               by Account 
);
// Calculate avg number of logins
let AccountAvg = (
    baseData
      // For each row, calculate all of the stats using series_stats_dynamic
      // then extract just the avg from that list. Convert that to a 
      // double then add it as a new column, avg
    | extend avg = todouble(series_stats_dynamic(dcount_Computer).avg ) 
);
AccountAvg;

// use last 7 days
// Now get the domain controllers
let myDCs = toscalar(
    ADAssessmentRecommendation
    | summarize makeset(Computer)
);
print myDCs;

//use last 7 days
// For the computer name we just got, get the domain accounts
let myDCs = toscalar(
    ADAssessmentRecommendation
    | summarize makeset(Computer)
);
let myDAs = toscalar(
    SecurityEvent
    | where EventID in (4672, 576)
    | where Computer in (myDCs)
    | summarize makeset(Account)
);
print myDAs;


// Now put it all together
let fromDate = ago(5d);
let thruDate = now();
let baseData = materialize(
    SecurityEvent
    | where EventID == 4624  // 4624 - An account was successfully logged on.
    | where AccountType == "User"
    | make-series dcount(Computer) default=0 
               on TimeGenerated in range(fromDate, thruDate, 1d) 
               by Account 
);
let AnomAccounts = (
    baseData
    | extend outliers = series_outliers(dcount_Computer) 
    | mvexpand dcount_Computer, TimeGenerated, outliers to typeof(double)
    | top 25 by outliers desc
    | project Account 
);
let AccountAvg = (
    baseData
    | extend avg = todouble(series_stats_dynamic(dcount_Computer).avg ) 
);
let myDCs = toscalar(
    ADAssessmentRecommendation
    | summarize makeset(Computer)
);
let myDAs = toscalar(
    SecurityEvent
    | where EventID in (4672, 576)
    | where Computer in (myDCs)
    | summarize makeset(Account)
);
SecurityEvent
  // 4624 - An account was successfully logged on.
| where EventID  == 4624
  // Where the account was in the list of anomolies
| where Account in (AnomAccounts)
  // Add a column to let us know if the accout is an admin
| extend IsAdmin = iff(Account in (myDAs), "yes", "no")
  // Get a distinct count of the computers by the account and
  // whether the user was an admin
| summarize CompCount = dcount(Computer) by Account, IsAdmin
  // Now join this to the table of averages for that account
| join kind=leftouter (
   AccountAvg 
) on Account
  // Remove the columns we no longer need
| project-away Account1, dcount_Computer, TimeGenerated ;       

// Now we have a list of accounts, whether that account is an admin,
// how many machines did they log into, and what is the average 
// number of machines they usually login to


//series_decompose for weekly seasonality
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 10.0, 15.0) - (((t%24)/10)*((t%24)/10)) // generate a series with weekly seasonality
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose(y)
| render timechart  ;

// with trend
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose(y)
| render timechart  ;

//series_decomposition for weekly seasonality with anomolies
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 10.0, 15.0) - (((t%24)/10)*((t%24)/10)) // generate a series with weekly seasonality
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose_anomalies(y)
| render timechart ;

//series_decomposition for weekly seasonality with anomolies
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose_anomalies(y)
| extend series_decompose_anomalies_y_ad_flag = 
series_multiply(10, series_decompose_anomalies_y_ad_flag) // multiply by 10 for visualization purposes
| render timechart  ;
;

// now with line fit parameter
// you can also adjust the threshold value say from 1.5 to 2.5
let ts=range t from 1 to 24*7*5 step 1 
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| summarize Timestamp=make_list(Timestamp, 10000),y=make_list(y, 10000);
ts 
| extend series_decompose_anomalies(y, 1.5, -1, 'linefit') // (series, threshold, seasonality (-1 auto), trend) - there are more check docs
| extend series_decompose_anomalies_y_ad_flag = 
series_multiply(10, series_decompose_anomalies_y_ad_flag) // multiply by 10 for visualization purposes
| render timechart;

// now lets try series_decompose_forecast to fill in the last week
let ts=range t from 1 to 24*7*4 step 1 // generate 4 weeks of hourly data
| extend Timestamp = datetime(2018-03-01 05:00) + 1h * t 
| extend y = 2*rand() + iff((t/24)%7>=5, 5.0, 15.0) - (((t%24)/10)*((t%24)/10)) + t/72.0 // generate a series with weekly seasonality and ongoing trend
| extend y=iff(t==150 or t==200 or t==780, y-8.0, y) // add some dip outliers
| extend y=iff(t==300 or t==400 or t==600, y+8.0, y) // add some spike outliers
| make-series y=max(y) on Timestamp in range(datetime(2018-03-01 05:00), datetime(2018-03-01 05:00)+24*7*5h, 1h); // create a time series of 5 weeks (last week is empty)
ts 
| extend y_forcasted = series_decompose_forecast(y, 24*7)  // forecast a week forward
| render timechart;

// lets find periodic (ie. weekly and daily)
print y=dynamic([80,139,87,110,68,54,50,51,53,133,86,141,97,156,94,149,95,140,77,61,50,54,47,133,72,152,94,148,105,162,101,160,87,63,53,55,54,151,103,189,108,183,113,175,113,178,90,71,62,62,65,165,109,181,115,182,121,178,114,170])
| project x=range(1, array_length(y), 1), y  
| render linechart;

// series_periods_detect
// Important
// The algorithm can detect periods containing at least 4 points and at most half of the series length.
// You should set the min_period a little below and max_period a little above the periods you expect to find in the time series. For example, if you have an hourly-aggregated signal, and you look for both daily > and weekly periods (that would be 24 & 168 respectively) you can set min_period=0.8*24, max_period=1.2*168, leaving 20% margins around these periods.
// The input time series must be regular, i.e. aggregated in constant bins (which is always the case if it has been created using make-series). Otherwise, the output is meaningless.
// 
print y=dynamic([80,139,87,110,68,54,50,51,53,133,86,141,97,156,94,149,95,140,77,61,50,54,47,133,72,152,94,148,105,162,101,160,87,63,53,55,54,151,103,189,108,183,113,175,113,178,90,71,62,62,65,165,109,181,115,182,121,178,114,170])
| project x=range(1, array_length(y), 1), y  
| project series_periods_detect(y, 0.0, 50.0, 2) //( array, min period, max period, number of periods
//|render linechart ;


// series_fir
// FIR Is Finite Impulse Repsonse time. It is typically used in digital 
// signal processing, such as that used in radio (especially amateur radio). 
// The finite indicates the array of values, if continued, would eventually
// dwindle down to a zero value.
// FIR analysis is a specialized discipline, however we can use FIR to assist
// with other more common processing needs. 

// Show a moving average of last 5 values
range t from bin(now(), 1h)-23h to bin(now(), 1h) step 1h
| summarize t=make_list(t)
| project id='TS', val=dynamic([0,0,0,0,0,0,0,0,0,10,20,40,100,40,20,10,0,0,0,0,0,0,0,0]), t
| extend 5h_MovingAvg=series_fir(val, dynamic([1,1,1,1,1])),
         5h_MovingAvg_centered=series_fir(val, dynamic([1,1,1,1,1]), true, true)
| render timechart;


range t from bin(now(), 1h)-23h to bin(now(), 1h) step 1h
| summarize t=makelist(t)
| project val=dynamic([10,20,30,40,5,6,7,8,0,10,20,40,100,40,20,10,20,9,7,20,80,10,5,1]), t
| extend 5h_MovingAvg=series_fir(val, dynamic([1,1,1,1,1])),
         5h_MovingAvg_centered=series_fir(val, dynamic([1,1,1,1,1]), true, true)
| mvexpand val, t, 5h_MovingAvg, 5h_MovingAvg_centered;         

// Show difference between current value and previous value
range t from bin(now(), 1h)-11h to bin(now(), 1h) step 1h
| summarize t=makelist(t)
| project t,value=dynamic([1,2,4,6,2,2,2,2,3,3,3,3])
| extend diff=series_fir(value, dynamic([1,-1]), false, false)
| mvexpand t, value, diff;

// Now to do something more useful. Get a list of High CPU Alerts for a 
// computer, then for each alert calculate the time since the previous 
// alert. Zero out the first row since that data would be misleading.
let filterOutBigValues = (val:real)
{
  iif( val > 10000000, 0.0, val)
};
let convertToMinutes = (val:real)
{
  // 1 sec = 10000000 ns
  round( (val / 10000000) / 60.0, 2)   
};
let convertMinToHours = (val:real)
{
  round(val / 60.0, 2)
};
Alert
| where TimeGenerated >= ago(90d)
| where AlertName == "Computers with high processor usage [Log alert on Log Analytics data]"
| where Computer == "InfraScaleVMs"
| project Computer 
        , TimeGenerated 
| order by TimeGenerated asc
| summarize tg=makelist(tolong(TimeGenerated)) by Computer
| extend diff=series_fir(tg,  dynamic([1, -1]), false, false)
| mvexpand tg, diff
| extend TimeGenerated = todatetime(tg) 
| extend diffInMinutes = convertToMinutes(diff)
| extend diffInHours = convertMinToHours(diffInMinutes)
| extend DifferenceInMinutes = filterOutBigValues(diffInMinutes)
       , DifferenceInHours =  filterOutBigValues(diffInHours)  
| project-away tg, diffInMinutes, diffInHours; 


//------------------------------------------------------------------------------
// series_iir
//------------------------------------------------------------------------------

// Similar to FIR, IIR is Infinite Impulse Response. Like FIR, it is most
// commonly used in signal processing. The major difference between FIR and IIR
// is that the range of values is assumed to go on infinately, rather than 
// than FIRs assumption it will be declining to zero 

// Like FIR, IIR can be used for similar functions. 

// Use IIR to calculate cumulative values
range t from bin(now(), 1h)-23h to bin(now(), 1h) step 1h
| summarize t=makelist(t)
| project val=dynamic([10,20,30,40,5,6,7,8,0,10,20,40,100,40,20,10,20,9,7,20,80,10,5,1]), t
| extend CumulativeTotal=series_iir(val, dynamic([1]), dynamic([1,-1])    )
| mvexpand val, t, CumulativeTotal         


// Here, we will determine the number of alerts for a computer on a given 
// day, then accumulate them using series_iir
Alert
| where TimeGenerated >= ago(90d)
| where AlertName == "Computers with high processor usage [Log alert on Log Analytics data]"
| where Computer == "InfraScaleVMs"
| summarize AlertCount=count() by bin(TimeGenerated, 1d) 
| order by TimeGenerated asc
| summarize ac=makelist(AlertCount), tg=makelist(TimeGenerated)
| extend CumulativeAlertCount = series_iir(ac,  dynamic([1]),  dynamic([1, -1]) )
| mvexpand tg, ac, CumulativeAlertCount
| extend AlertDate = format_datetime(todatetime(tg), 'yyyy-MM-dd') 
| project AlertDate, DailyAlertCount = ac, CumulativeAlertCount ;


// With a slight variation to our query, we can now render this as a chart,
// making it much easier to spot any spikes in alert counts
Alert
| where TimeGenerated >= ago(90d)
| where AlertName == "Computers with high processor usage [Log alert on Log Analytics data]"
| where Computer == "InfraScaleVMs"
| summarize AlertCount=count() by bin(TimeGenerated, 1d) 
| order by TimeGenerated asc
| summarize ac=makelist(AlertCount), tg=makelist(TimeGenerated)
| extend CumulativeAlertCount = series_iir(ac,  dynamic([1]),  dynamic([1, -1]) )
| mvexpand tg, ac, CumulativeAlertCount
| extend AlertDate = todatetime(tg)
       , CumulativeAlertCount = toint(CumulativeAlertCount) 
| project AlertDate, CumulativeAlertCount
| render timechart 

//------------------------------------------------------------------------------
// series_fit_line
//------------------------------------------------------------------------------

// series_fit_line performs a linear regression on a series. It returns
// multiple values:
// RSquare: Standard measure of the fit of quality, in a range of 0 to 1. 
//          the closer to 1, the better the fit.
// Slope:   Slope of the approximated line
// Variance: The variance of the input data
// RVariance: Residual variance (the variance between the input data)
// Interception: Interception of the approcimated line
// Line_Fit: Numericial array holding the values of the best fit line.
//           Typically used for charting.
range x from 1 to 1 step 1
| project x = range(bin(now(), 1h)-11h, bin(now(), 1h), 1h)
        , y = dynamic([2,5,6,8,11,15,17,18,25,26,30,30])
| extend (RSquare,Slope,Variance,RVariance,Interception,LineFit) = series_fit_line(y);

// Render this as charted data
// y represents the actual value
// LineFit represents the best fit "predicted" value
range x from 1 to 1 step 1
| project x=range(bin(now(), 1h)-11h, bin(now(), 1h), 1h)
        , y=dynamic([2,5,6,8,11,15,17,18,25,26,30,30])
| extend (RSquare,Slope,Variance,RVariance,Interception,LineFit) = series_fit_line(y)
| render timechart;


// Let's render a chart based on the total memory (in kb) used per hour
Perf
| where TimeGenerated > ago(1d)
| where CounterName == "Used Memory kBytes" 
| where CounterValue > 0
| make-series TotalMemoryUsed=sum(CounterValue) default=0 
           on TimeGenerated 
           in range(ago(1d), now(), 1h) 
           by Computer
| extend (RSquare,Slope,Variance,RVariance,Interception,LineFit)=series_fit_line(TotalMemoryUsed)
| render timechart;


//------------------------------------------------------------------------------
// series_fit_2lines
//------------------------------------------------------------------------------

// series_fit_2lines takes the input data and splits the collection (it uses
// the terms 'right side' and 'left side' to differentiate). It then
// performs an anaylsis on each part of the data. The best split is the one
// with the maximized RSquare. It will return the best values, but you
// can also return the right and left side values if you want

// Return all values
range x from 1 to 1 step 1
| project x = range(bin(now(), 1h)-11h, bin(now(), 1h), 1h)
        , y = dynamic([2,5,6,8,11,15,17,18,25,26,30,30])
| extend ( RSquare
         , SplitIdx
         , Variance
         , ResidualVariance
         , LineFit
         , right_rsquare
         , right_slope
         , right_interception
         , right_variance
         , right_rvariance
         , left_rsquare
         , left_slope
         , left_variance
         , left_rvarience
         ) = series_fit_2lines(y);

// Return only the key values
range x from 1 to 1 step 1
| project x = range(bin(now(), 1h)-11h, bin(now(), 1h), 1h)
        , y = dynamic([2,5,6,8,11,15,17,18,25,26,30,30])
| extend ( RSquare
         , SplitIdx
         , Variance
         , ResidualVariance
         , LineFit
         ) = series_fit_2lines(y);


// Let's render a chart based on the total memory (in kb) used per hour
Perf
| where TimeGenerated > ago(1d)
| where CounterName == "Used Memory kBytes" 
| where CounterValue > 0
| make-series TotalMemoryUsed=sum(CounterValue) default=0 
           on TimeGenerated 
           in range(ago(1d), now(), 1h) 
           by Computer
| extend (RSquare,SplitIdx,Variance,RVariance,LineFit)=series_fit_2lines(TotalMemoryUsed)
| render timechart ;

   